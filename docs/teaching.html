<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Books save me a lot</title>

<script src="site_libs/header-attrs-2.28/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Homepage</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="hello.html">About</a>
</li>
<li>
  <a href="teaching.html">Reading</a>
</li>
<li>
  <a href="https://yuuuulu.github.io/stat.pre/">Projects and Contests</a>
</li>
<li>
  <a href="https://yuuuulu.github.io/Math-s-interesting-things/">Foundation Mathematical Learning</a>
</li>
<li>
  <a href="resource.html">Progress Everyday</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Books save me a lot</h1>

</div>


<div
id="the-lady-tasting-teahow-statistics-revolutionized-science-in-the-twentieth-century"
class="section level2">
<h2>The Lady Tasting Tea：How Statistics Revolutionized Science in the
Twentieth Century</h2>
<div id="motivation" class="section level3">
<h3>Motivation</h3>
<ul>
<li>To understand God’s thoughts, we must study statistics, for these
are the measure of his purpose. —Florence Nightingale</li>
</ul>
<p>She used statistical methods to improve healthcare during the Crimean
War, demonstrating the power of data visualization.</p>
</div>
<div id="error-function" class="section level3">
<h3>Error function</h3>
<p>Laplace did not need God in his formulation, he did need something he
called the “error function”.</p>
<p>–Predictions do not fit observations exactly (eg. planets and comets
predicted by formulas due to perturbations in the earth’s atmosphere/
human error)</p>
<p>–use error function to account for slight discrepancies between the
observed and the predicted</p>
<p>–early 19th-century science was in the grip of philosophical
determinism–the belief that everything that happens is determined in
advance by the initial conditions of the universe and the mathematical
formulas that describe its motions</p>
<p>–more precise measurement not exactly leads to less error—-instead
inversely. Newton and Laplace had used were proving to be only rough
approximations.</p>
<p>—new paradigm: statistical model of reality</p>
<p>–by the end of the 20th century almost all of science had shifted to
using statistical models.</p>
<p>ideas and expression have drifted into the popular vocabulary</p>
<p>–profound shift in philosophical view: what are these statistical
models? How did they come about? What do they mean in real life?</p>
</div>
<div id="mathematical-ideas" class="section level3">
<h3>3 mathematical ideas</h3>
<div id="randomness" class="section level4">
<h4>randomness</h4>
<ul>
<li><p>past: unpredictablity: one cannot go searching for something
which is found at random</p></li>
<li><p>morden scientists: <strong>probability distribution</strong>:
allows us to put constraints on this randomness and gives us a limited
ability to predict future but random events.—-randomness have a
structure that can be described mathematically.</p></li>
</ul>
</div>
<div id="probability" class="section level4">
<h4>Probability</h4>
<p>From Aristotle:“it is the nature of probability that improbable
things will happen”</p>
<p>19th century: consisted primarily of sophisticated tricks but lacked
a sold theoretical foundation</p>
<p>Early 20th Century: Ronald A. Fisher (rejected by Pearson in the
Biometrika and then went to agricultral station) revolutionized
statistics with his work on experimental design, MLE, multiple
comparison, analysis of variance (ANOVA). Jerzy Neyman and Egon Pearson
developed hypothesis testing and confidence intervals.</p>
<p>Mid-20th Century: the development of computers in the 1950s and 1960s
enabled more complex data analysis</p>
<p>Bernoullis –Fermat De Moivre(insert Calculus) discern some deep
fundabemtal theorems–“laws of large numbers”</p>
<p>Pascal</p>
<p>games of chance, counting equally probable events</p>
</div>
</div>
</div>
<div id="statistics" class="section level1">
<h1>statistics</h1>
<p>population <span class="math inline">\(f(x;\theta)\)</span> and <span
class="math inline">\(\theta\)</span> is unknown</p>
<div id="oldclassical-bayesian-estimation-moment-estimation"
class="section level2">
<h2>old/classical: Bayesian estimation; moment estimation</h2>
</div>
<div id="moment-estimation" class="section level2">
<h2>Moment estimation</h2>
<ul>
<li>For the moment estimation, we do not know the density so we could
not calculate the expectation directly. Instead, we use LLN to
approximate the expectation with sample mean. We then have</li>
</ul>
<p><span class="math display">\[
m_1(\theta) = E[X] \approx \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>then <span class="math display">\[
\hat \theta = m_1^{-1}(\frac{1}{n} \sum_{i=1}^n X_i)
\]</span></p>
<p>For the multiple parameter case, we could use multiple moments to
estimate the parameters. (k equations to solve k unknown parameters).
Here we do not know the precise distribution, so we could not calculate
the moments directly. Instead, we use LLN to approximate the moments
with sample moments.</p>
<p>Suppose <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>
are iid samples from the distribution with parameters <span
class="math inline">\(f(x;\theta_1, \theta_2, \ldots, \theta_k)\)</span>
then by LLM.</p>
<p>We then have approximatedmoment equation:</p>
<p><span class="math display">\[
E[X^j] = m_j(\theta_1, \theta_2, \ldots, \theta_k) \approx \frac{1}{n}
\sum_{i=1}^n X_i^j, j=1,2,\ldots,k
\]</span></p>
<p>then we could solve the equations to get the estimators.</p>
<p>The solution <span class="math inline">\(\hat \theta_1, \hat
\theta_2, \ldots, \hat \theta_k\)</span> are the moment estimators of
the parameters.</p>
</div>
<div id="bayesian-estimation" class="section level2">
<h2>Bayesian estimation</h2>
<p><span class="math inline">\(f(x|\theta)\)</span> is regarded as
conditional density and the unknown parameter <span
class="math inline">\(\theta\)</span> is the condition. <span
class="math inline">\(\theta\)</span> is a random variable with prior
distribution <span class="math inline">\(\pi(\theta)\)</span>.</p>
<p><span class="math inline">\((X, \theta)\)</span> has joint
distribution <span
class="math inline">\(f(x,\theta)=f(x|\theta)\pi(\theta)\)</span></p>
<p>Then we get the marginal distribution of <span
class="math inline">\(X\)</span>:</p>
<p><span class="math inline">\(f_X(x) = \int_{-\infty}^{+\infty}
f(x|\theta)\pi(\theta)d\theta=\int_{-\infty}^{+\infty}
f(x;\theta)\pi(\theta)d\theta\)</span></p>
<p>the Bayesian formula is</p>
<p><span class="math display">\[ \frac {P(A|C_i)P(C_i)}{\sum_j
P(A|C_j)P(C_j)} =P(C_i|A)\]</span></p>
<p><span class="math display">\[
f(\theta|x)=\frac{f(x|\theta)\pi(\theta)}{f_X(x)} =
\frac{f(x;\theta)\pi(\theta)}{\int_{-\infty}^{+\infty}
f(x;\theta)\pi(\theta)d\theta}
\]</span></p>
<p>Then above we have used Bayes theorem to get the posterior
distribution of <span class="math inline">\(\theta\)</span> given <span
class="math inline">\(X=x\)</span></p>
<p>This is the updation of your belief or learning.</p>
<p>Bayesian statistics is unclear and ambiguous, but leaving us with no
choice but to incorporate the most fundamental knowledge we have
（不清不楚，不得不用）. We then update our understanding through
experimental data. If it cannot be accomplished in one attempt, we
iterate multiple times—eventually, we will gain a solid understanding of
the parameters. The learning process is one of repeated iteration; there
is no need to expect it to be completed in a single attempt.</p>
<p>When we do not know every knowledge about the model, we could use the
prior of uniform distribution to represent our ignorance. That is,
everyone is equally likely.</p>
<p>for <span class="math inline">\(f(x;\theta)\)</span>, we could
calculate the expectation ; mode; CI</p>
<p>modern statistics</p>
<p><span class="math inline">\(L(\theta) = E[R(\theta,X)]\)</span></p>
<p><span class="math inline">\(\hat \theta = argmin_\theta
L(\theta)\)</span></p>
<p>where <span class="math inline">\(R(\theta,X)\)</span> is the loss
function</p>
</div>
<div id="fisher-mle" class="section level2">
<h2>Fisher: MLE</h2>
<p><strong>Fisher ambitious ：
不想依赖于贝叶斯，选prior的时候不能包含个人的任何认知，那就选客观的uniform分布</strong>
(Fisher ambitious: Not wanting to rely on Bayesian, when choosing prior,
no personal cognition can be included. Then choose the objective uniform
distribution)</p>
<p><span class="math inline">\(\pi (\theta|x) {d\theta}=
d\theta\)</span> uniform</p>
<p><span class="math inline">\(f(\theta|x_1, x_2, \ldots, x_n) =
\frac{f(x_1, x_2, \ldots, x_n;\theta)\pi(\theta)}{\int f(x_1, x_2,
\ldots, x_n;\theta)\pi(\theta)d\theta}\)</span> = <span
class="math inline">\(L(\theta)\)</span></p>
<p><span class="math display">\[
= \frac{f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta) }{\int
f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta)  d\theta}=L(\theta)
\]</span></p>
<p>Mode:</p>
<p>Find maximum of <span class="math inline">\(f(\theta|x_1, x_2,
\ldots, x_n)\)</span> i.e. L(<span
class="math inline">\(\theta\)</span>)</p>
<p><span class="math inline">\(\hat \theta = argmax_\theta
L(\theta)=argmax_\theta \frac{f(x_1;\theta)f(x_2;\theta)\ldots
f(x_n;\theta)}{f_X(x_1,x_2,..x_n)}\)</span></p>
<p><span class="math inline">\(= argmax_\theta
f(x_1;\theta)f(x_2;\theta)\ldots f(x_n;\theta)=argmax_\theta
L(\theta)\)</span> this is the likelihood function</p>
<p>When Fisher wanted to publish his idea to the Royal Society, he had
been harshly commented through the discussion of the Royal Society. This
is a good tradition but nowadays’ conference does ntot keep this. This
means now they just have a presentation but the comments will not appear
in the published paper.</p>
<p>then we could use log likelihood to simplify the calculation of the
derivative to get the maximum likelihood estimator.</p>
<p>modern: Loss function</p>
</div>
</div>
<div id="modern-statistics" class="section level1">
<h1>Modern Statistics</h1>
<p>Loss function is a “scanning apparatus”. It will take special value
at the true parameter such as 0 or minimum point.</p>
<div id="empirical-kullback-leibler-divergence-loss-and-mle"
class="section level2">
<h2>Empirical Kullback-Leibler Divergence Loss and MLE</h2>
<p>(Empirical loss means that since we do not know the true
distribution, we could use the sample to approximate the loss function.
)</p>
<p>Consider the empirical loss function：</p>
<p><span class="math display">\[l(\theta) = \mathbb{E}_{\Theta} \left[
\frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)}
\right]\]</span></p>
<p><span class="math display">\[= \int \frac{f(x;\theta_0)}{f(x;\theta)}
\log \frac{f(x;\theta_0)}{f(x;\theta)} f(x;\theta) dx\]</span></p>
<p><span class="math display">\[= \int \left[ \log
\frac{f(x;\theta_0)}{f(x;\theta)} \right] f(x;\theta_0) dx\]</span></p>
<p><span class="math display">\[= \mathbb{E}_{\Theta_0} \left[ \log
\frac{f(x;\theta_0)}{f(x;\theta)} \right]\]</span></p>
<p><span class="math display">\[\approx \frac{1}{n} \sum_{i=1}^{n} \log
\frac{f(x_i;\theta_0)}{f(x_i;\theta)}\]</span></p>
<p><span class="math display">\[= \frac{1}{n} \sum_{i=1}^{n} \log
f(x_i;\theta_0) - \frac{1}{n} \sum_{i=1}^{n} \log
f(x_i;\theta)\]</span></p>
<p><span class="math display">\[\text{Argmin}(\hat{\theta}) =
\text{Argmin}\left[ -\frac{1}{n} \sum_{i=1}^{n} \log f(x_i;\theta)
\right]\]</span></p>
<p><span class="math display">\[= \text{Argmax}\left[ \sum_{i=1}^{n}
\log f(x_i;\theta) \right]\]</span></p>
<p><span class="math display">\[= \text{Argmax}\left[ \log
\prod_{i=1}^{n} f(x_i;\theta) \right]\]</span></p>
<div id="kl-divergence-property" class="section level3">
<h3>KL-divergence Property</h3>
<p>Let <span class="math inline">\(E_{\theta}\)</span> be the
expectation of <span class="math inline">\(f_{\theta}(x)\)</span>.</p>
<p>the true model <span
class="math inline">\(f(x;\theta_0)\)</span>，and the proposed model is
<span class="math inline">\(f(x,\theta)\)</span>。</p>
<p>KL (entropy) divergence:</p>
<p><span class="math display">\[\mathbb{E}_{\theta} \left[
\frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)}
\right]\]</span></p>
<p>Let <span class="math inline">\(\varphi(x) = x \log x\)</span>，then
<span class="math inline">\(\varphi&#39;(x) = \log x + 1\)</span>，<span
class="math inline">\(\varphi&#39;&#39;(x) = \frac{1}{x} &gt;
0\)</span>，</p>
<p>So <span class="math inline">\(\varphi(x)\)</span> is a convex
function over <span class="math inline">\((0,+\infty)\)</span>.。</p>
<p>By Jensen inequality：</p>
<p><span class="math display">\[\varphi\left( \mathbb{E}_{\theta} \left[
\frac{f(x;\theta_0)}{f(x;\theta)} \right] \right) \leq
\mathbb{E}_{\theta} \left[ \varphi\left(
\frac{f(x;\theta_0)}{f(x;\theta)} \right) \right]\]</span></p>
<p><span class="math display">\[\mathbb{E}_{\theta} \left[
\frac{f(x;\theta_0)}{f(x;\theta)} \right] = \int_{-\infty}^{\infty}
\frac{f(x;\theta_0)}{f(x;\theta)} f(x;\theta) dx\]</span></p>
<p><span class="math display">\[= \int_{-\infty}^{\infty} f(x;\theta_0)
dx = 1\]</span></p>
<p>Thus</p>
<p><span class="math display">\[\varphi(1) = 1 \times \log 1 = 0 \leq
\mathbb{E}_{\theta} \left[ \frac{f(x;\theta_0)}{f(x;\theta)} \log
\frac{f(x;\theta_0)}{f(x;\theta)} \right]\]</span></p>
<p>Loss function</p>
<p><span class="math display">\[l(\theta) = \mathbb{E}_{\theta} \left[
\frac{f(x;\theta_0)}{f(x;\theta)} \log \frac{f(x;\theta_0)}{f(x;\theta)}
\right] \geq 0\]</span></p>
<p>and <span class="math inline">\(l(\theta_0) = 0\)</span>，so <span
class="math inline">\(\theta_0\)</span> is <span
class="math inline">\(l(\theta)\)</span>’s minimum point</p>
<p>Empirical estimators are:</p>
<p><span class="math display">\[l(\theta) \approx \frac{1}{n}
\sum_{i=1}^n \frac{f(x_i;\theta_0)}{f(x_i;\theta)} \log
\frac{f(x_i;\theta_0)}{f(x_i;\theta)}\]</span></p>
<p><strong>Conclusion: Maximum likelihood estimation is equivalent to
minimizing the KL divergence, which provides the theoretical basis of
information theory for MLE. </strong></p>
<p>DeepSeek loss function—Feed the data - gradient descent—LLM</p>
<p>statistical distribution</p>
<p>distribution function used to examine the question, like Laplace’s
error function but much more complicated, using probability theory to
describe what might be expected from future data taken at random from
the same population of people</p>
</div>
</div>
<div id="section" class="section level2">
<h2></h2>
</div>
<div
id="心理统计日常生活中的统计推理原书第3版作者杰弗里o.贝内特威廉l.布里格斯马里奥f.特里奥拉psychostatistics-the-original-book-of-statistical-reasoning-in-everyday-life-3rd-edition-by-jeffrey-o.-bennett-william-l.-briggs-mario-f.-triola"
class="section level2">
<h2>《心理统计(日常生活中的统计推理原书第3版)》(作者杰弗里O.贝内特、威廉L.布里格斯、马里奥F.特里奥拉)(Psychostatistics
(The Original Book of Statistical Reasoning in Everyday Life, 3rd
Edition) by Jeffrey O. Bennett, William L. Briggs, Mario F.
Triola))</h2>
<p>Reading this kind of books to cultivate my ability of telling story,
which help me understand statistics knowledge in a deeper depth.</p>
</div>
<div id="ten-great-ideas-about-chance" class="section level2">
<h2>TEN GREAT IDEAS ABOUT CHANCE</h2>
<p>Written by PERSI DIACONIS &amp; BRAIN SKYRMS</p>
<div id="measurement" class="section level3">
<h3>Measurement</h3>
<p>Nov.1st</p>
</div>
<div id="judegement" class="section level3">
<h3>Judegement</h3>
</div>
</div>
<div id="statistical-inference-by-george-casella-and-roger-l.-berger"
class="section level2">
<h2>Statistical inference by George Casella and Roger L. Berger</h2>
<p><span class="citation">(Casella and Berger 2024)</span> likes Sir
Arthur Conan Doyle and Sherlock Holmes, and I also like them (an old
memory..). Finding some stories in a theory book is a good way to make
the book more interesting. I also like the way they use examples to
explain theorems, which makes it easier to understand. I think this is a
good way to learn statistics.Sir Arthur Conan Doyle and Sherlock Holmes
are indeed timeless classics. Using their stories to explain statistical
theories is like adding a dollop of intrigue to your math. Image Holmes
cracking a case with Bayesian inference or predicting the next crime
using regression analysis. It s like transforming a dry textbook into a
thrilling adventure! But yeah, this book is definitely not a walk in the
park. It s more like a marathon through a maze of numbers and theorems.
—yes, often really difficult for me. Just remember, even Holmes had his
tough cases, but he always figured them out in the end. So, keep at it,
and maybe one day you ll be the Sherlock of statistics, solving the most
baffling statistical mysteries</p>
</div>
<div
id="differential-equations-with-applications-and-historical-notes-by-george.-f-simmons"
class="section level2">
<h2>Differential Equations With Applications And historical Notes by
George. F Simmons</h2>
<div
id="preface-really-attracts-me-a-lot-since-the-author-loves-bell-curve-very-much"
class="section level3">
<h3>Preface really attracts me a lot (since the author loves bell curve
very much!!!)</h3>
<ul>
<li>how the differential equation for this curve ariases from very
simple considerations and can be solved to obtain the equation of the
curve itself</li>
</ul>
</div>
</div>
<div id="wisdom-of-the-west" class="section level2">
<h2>Wisdom of the west</h2>
<ul>
<li><p>“No man’s knowledge here can go beyond his experience.”</p>
<ul>
<li>Gain experience beyond immediate theoretical boundaries—to immerse
themselves in the problems of biology, economics, medicine, and more. By
doing so, they expand their understanding and ensure their work remains
benefit.</li>
</ul></li>
<li><p>“Once a science becomes solidly groundly, it proceeds more or
less independently”</p>
<ul>
<li>In this case, the philosophy meaning of bioststistics still stands
well.</li>
</ul></li>
</ul>
</div>
<div id="martin-buber-i-and-thou" class="section level2">
<h2>Martin Buber I and Thou</h2>
</div>
<div
id="good-quotes-with-my-feeling-relavent-to-my-understanding-of-my-major"
class="section level2">
<h2>Good Quotes with My Feeling (relavent to my understanding of my
major)</h2>
<div
id="it-can-scarcely-be-denied-that-the-supreme-goal-of-all-theory-is-to-make-the-irreducible-basic-elements-as-simple-and-as-few-as-possible-without-having-to-surrender-the-adequate-representation-of-a-single-datum-of-experience."
class="section level3">
<h3>“It can scarcely be denied that the supreme goal of all theory is to
make the irreducible basic elements as simple and as few as possible
without having to surrender the adequate representation of a single
datum of experience.”</h3>
<p>不应否认，任何理论的终极目标都是尽可能让不可简化的基本元素变得更加简单且更少，但也不能放弃对任何一个单一经验数据的充分解释。—Thanks
to the book “The Model Thinker” written by Scott Page to let me know
this sentence.</p>
<p>“It can scarcely be denied that the supreme goal of all theory is to
make the irreducible basic elements as simple and as few as possible
without having to surrender the adequate representation of a single
datum of experience.” Einstein’s words have given me the ultimate goal
in mastering a field of study (for me, statistics), constantly reminding
me to keep moving forward. This is also an example of the power that
books bring to me.—Thanks to the book “The Model Thinker” written by
Scott Page to let me know this sentence.</p>
<ul>
<li><p>“Youth means a temperamental predominance of courage over
timidity of the appetite, for adventure over the love of ease.”</p>
<ul>
<li>This quote is from the poem “Youth”, which could serve as a call to
young statisticians to be bold: venture out of the comfort of
theoretical work, take risks in collaborating across fields, and engage
with messy, real-world data. This spirit of adventure is what will keep
the field vibrant and ensure its continued relevance in the evolving
landscape of science and technology.</li>
</ul></li>
<li><p>“Theorem go, theorem come, only examples lie forever.”</p>
<ul>
<li><p>This highlights the enduring value of real-world impact over
theoretical intricacies.It is through practical examples, solving real
problems, that statistical methods prove their lasting worth.</p>
<p>(Thinking more concisely and wisely by leveraging examples in our
beautiful nature.)</p>
<p>(Examples remain fixed points of reference, giving continuity in the
landscape of mathematical thought.)</p></li>
</ul></li>
<li><p>Feeling: During university, when laying the foundation of
knowledge, it’s essential to focus on theoretical fundamentals,
including mathematical analysis (such as Fourier analysis, real
analysis, complex analysis, and functional analysis), and a profound
understanding of linear algebra. As statisticians venture out to apply
their skills, they should carry these foundational principles with them,
dedicating themselves more to real-world applications that benefit
society. For me, it is especially in fields like medicine, which is
particularly relevant for a student of biostatistics.Balancing deep
theoretical work with practical applications during university life of a
statistics- majored student will keep the field dynamic and ensure its
continued relevance in the evolving landscape of science and
technology.</p></li>
<li><p>“The most incomprehensible thing about the universe is that it is
comprehensible.” –Albert Einstein</p>
<ul>
<li>the ability to make sense of complex data is at the heart of what
makes the discipline so valuable. not only in embracing theoretical
rigor, which is inherently demanding and critical, but also in seeking
out opportunities to collaborate across disciplines and tackle
real-world data challenges.</li>
</ul></li>
</ul>
</div>
</div>
<div id="dealing-with-living-people-when-studying"
class="section level2">
<h2>Dealing with living people when studying</h2>
<p>It is really a lucky thing to learn from the writer of a book or a
paper face-to-face to discuss some problems. Chatting with them could
really eliminate the confusion. Even though some good mathematicians or
statisticians are also really good at wrting because of their humanistic
quality, it is still not the same as communicating directly. For
example, learning from teachers during office hour is really more
helpful than reading their notes only, especially for the deeper
thinking notes.</p>
</div>
<div id="reference-list" class="section level2 unnumbered">
<h2 class="unnumbered">reference list</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
entry-spacing="0">
<div id="ref-casella2024" class="csl-entry">
Casella, George, and Roger Berger. 2024. <em>Statistical Inference</em>.
CRC Press.
</div>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
